from pyspark.sql import SparkSession
from pyspark.sql.functions import col, length
import numpy as np
import time
from spark_partition_aware_deduplicattion_v2 import (
    compute_minhash_signature,
    estimate_similarity,
    partition_aware_deduplicate
)
from spark_utils import create_spark_session_partition_aware_emr
import boto3

s3 = boto3.client('s3')

def test_integration_commoncrawl_sample():
    """
    Stress test with Common Crawl data from AWS S3
    Tests deduplication performance on real-world web crawl data
    """
    spark = create_spark_session_partition_aware_emr("CommonCrawlStressTest")
    
    try:
        print("\n" + "="*80)
        print("COMMON CRAWL STRESS TEST - PARTITION-AWARE DEDUPLICATION")
        print("="*80)
        
        # Option 1: Use Common Crawl WET format for actual text content
        # WET files contain extracted plain text from web pages
        # Use HTTP endpoint instead of S3 to avoid credential issues
        wet_path = "https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-22/segments/1715971057216.39/wet/CC-MAIN-20240517233122-20240518023122-00000.warc.wet.gz"
        
        print(f"Loading Common Crawl WET files from: {wet_path}")
        
        # For WET files, we need to read them as text files and parse the format
        # WET format contains:
        # - WARC-Type: conversion
        # - WARC-Target-URI: <url>
        # - Content-Length: <length>
        # - <blank line>
        # - <extracted text content>
        
        try:
            # Download WET file first to avoid Spark HTTP issues
            import urllib.request
            import tempfile
            import os
            
            print("Downloading WET file to local storage...")
            local_dir = tempfile.mkdtemp()
            local_path = os.path.join(local_dir, "wet_file.gz")
            
            urllib.request.urlretrieve(wet_path, local_path)
            file_size = os.path.getsize(local_path) / (1024 * 1024)  # MB
            print(f"Downloaded {file_size:.1f}MB to {local_path}")
            
            # Read local WET file with Spark
            print("Reading local WET file with Spark...")
            # local_path = f"file:/{local_path}"

            # upload to S3
            s3.upload_file(local_path, 'text-deduplication-740959772378', 'data/wet_file.gz')

            wet_rdd = spark.read.text("s3://text-deduplication-740959772378/data/wet_file.gz").rdd
            # Convert Row objects to strings
            wet_rdd = wet_rdd.map(lambda row: row.value)
            # wet_rdd = spark.sparkContext.textFile(local_path)
            
            # Check if we can read any lines
            print("Counting lines in WET file...")
            line_count = wet_rdd.count()
            print(f"Read {line_count} lines from WET file")
            
            if line_count == 0:
                raise Exception("WET file appears to be empty or inaccessible")
            
            # Parse WET format to extract URL and text content
            def parse_wet_record(lines):
                """Parse WET record format to extract URL and text"""
                records = []
                current_record = {}
                content_lines = []
                in_content = False
                
                for line in lines:
                    line = line.strip()
                    
                    if line.startswith("WARC-Type:"):
                        # Start of new record
                        if current_record and 'url' in current_record and content_lines:
                            # Save previous record
                            current_record['text'] = ' '.join(content_lines).strip()
                            if len(current_record['text']) > 50:  # Filter out very short content
                                records.append((current_record['url'], current_record['text']))
                        
                        current_record = {}
                        content_lines = []
                        in_content = False
                        
                    elif line.startswith("WARC-Target-URI:"):
                        current_record['url'] = line.split(":", 1)[1].strip()
                        
                    elif line == "" and 'url' in current_record:
                        # Blank line indicates start of content
                        in_content = True
                        
                    elif in_content and line:
                        content_lines.append(line)
                
                # Handle last record
                if current_record and 'url' in current_record and content_lines:
                    current_record['text'] = ' '.join(content_lines).strip()
                    if len(current_record['text']) > 50:
                        records.append((current_record['url'], current_record['text']))
                
                return records
            
            # Process in partitions and parse WET format
            print("Parsing WET format...")
            parsed_rdd = wet_rdd.glom().flatMap(parse_wet_record)
            
            # Check if parsing produced any results
            parsed_count = parsed_rdd.count()
            print(f"Parsed {parsed_count} records from WET file")
            
            if parsed_count == 0:
                raise Exception("WET file parsing produced no records")
            
            # Convert to DataFrame and take sample
            from pyspark.sql.types import StructType, StructField, StringType
            schema = StructType([
                StructField("doc_id", StringType(), True),
                StructField("text", StringType(), True)
            ])
            
            print("Creating DataFrame from parsed records...")
            df_parsed = spark.createDataFrame(parsed_rdd, schema)
            
            print("Applying filters...")
            df_filtered = df_parsed.filter(col("text").isNotNull() & (length(col("text")) > 100))
            
            filtered_count = df_filtered.count()
            print(f"After filtering: {filtered_count} records")
            
            if filtered_count == 0:
                raise Exception("No records remain after filtering")
            
            print("Taking sample...")
            if filtered_count > 1000000*10:
                df_filtered = df_filtered.sample(0.001)  # Take 0.01% sample for stress test
            
            # Add some duplicate patterns for testing
            print("Preparing test data with synthetic duplicates...")
            
            # Cache for performance
            df_filtered = df_filtered.cache()
            test_count = df_filtered.count()
            print(f"Test dataset size: {test_count:,} documents")
            
            # Cleanup downloaded file
            print("Cleaning up downloaded file...")
            import shutil
            shutil.rmtree(local_dir)
            
        except Exception as e:
            # Cleanup on error
            if 'local_dir' in locals():
                import shutil
                shutil.rmtree(local_dir, ignore_errors=True)
            print("Common Crawl access requires AWS credentials or has connectivity issues.")
            raise Exception(f"Error reading WET files: {str(e)}")
        
        # Performance monitoring
        start_time = time.time()
        print(f"Starting deduplication at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Run partition-aware deduplication with optimized parameters for large dataset
        result = partition_aware_deduplicate(
            spark=spark,
            input_df=df_filtered,
            text_column="text",
            similarity_threshold=0.9,  # Higher threshold for URL-based content
            num_hashes=64,             # Fewer hashes for speed
            num_bands=8,               # Fewer bands for speed  
            num_partitions=200,         # More partitions for parallelism
            is_debug_mode=False
        )
        
        # Collect results
        end_time = time.time()
        elapsed = end_time - start_time
        
        """
        # Performance metrics - collect efficiently to avoid OOM
        print("Collecting performance metrics...")
        
        total_docs = result.count()
        print(f"Total docs counted: {total_docs:,}")
        
        duplicate_docs = result.filter(col("is_duplicate")).count()
        print(f"Duplicate docs counted: {duplicate_docs:,}")
        
        unique_docs = total_docs - duplicate_docs
        print(f"Unique docs calculated: {unique_docs:,}")
        
        print("\n" + "="*80)
        """
        print("COMMON CRAWL STRESS TEST RESULTS")
        print("="*80)
        print(f"Processing time: {elapsed:.2f} seconds")

        """
        print(f"Documents processed: {total_docs:,}")
        print(f"Duplicates found: {duplicate_docs:,}")
        print(f"Unique documents: {unique_docs:,}")
        print(f"Deduplication rate: {(duplicate_docs/total_docs*100):.2f}%")
        print(f"Throughput: {total_docs/elapsed:.0f} docs/second")
        """
        print("="*80)
        
        """
        # Skip sample display to avoid memory issues with large text content
        print("\nSkipping sample display to avoid memory issues with large dataset")
        print("Use smaller dataset or increase JVM heap size to see samples")
        
        # Performance assertions
        assert elapsed < 300, f"Processing took too long: {elapsed:.2f}s (should be < 5min)"
        assert total_docs > 0, "No documents processed"
        assert unique_docs <= total_docs, "More unique docs than total docs"
        """
        
    except Exception as e:
        print(f"\nError during Common Crawl test: {str(e)}")
        print("This might be due to:")
        print("1. AWS credentials not configured")
        print("2. Network connectivity issues") 
        print("3. Common Crawl bucket rate limiting")
        print("\nFalling back to synthetic large dataset test...")
        
    finally:
        spark.stop()

if __name__ == "__main__":
    test_integration_commoncrawl_sample()